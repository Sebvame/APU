# ğŸ“š APU - Apuntes IA

APU (Apuntes IA) es un asistente conversacional inteligente diseÃ±ado para ayudar a estudiantes a consultar y comprender sus apuntes acadÃ©micos en formato IEEE. Utiliza tÃ©cnicas de Retrieval-Augmented Generation (RAG) para combinar bÃºsqueda semÃ¡ntica con generaciÃ³n de lenguaje natural.

![APU Logo](https://img.shields.io/badge/APU-Apuntes%20IA-blue?style=for-the-badge&logo=book)
![Python](https://img.shields.io/badge/Python-3.8+-green?style=flat-square&logo=python)
![License](https://img.shields.io/badge/License-MIT-yellow?style=flat-square)

## ğŸŒŸ CaracterÃ­sticas

- **ğŸ” BÃºsqueda Inteligente**: Encuentra informaciÃ³n relevante en tus documentos usando bÃºsqueda semÃ¡ntica
- **ğŸ’¬ Chat Natural**: Interfaz conversacional estilo NotebookLM
- **ğŸ“Š AnÃ¡lisis Contextual**: Comprende y explica conceptos complejos
- **ğŸŒ BÃºsqueda Web**: Complementa con informaciÃ³n actualizada de internet (cuando lo solicites)
- **ğŸ“ Citas y Referencias**: Siempre indica las fuentes de informaciÃ³n
- **ğŸ’¾ Memoria de SesiÃ³n**: Mantiene el contexto de la conversaciÃ³n
- **ğŸ“¤ ExportaciÃ³n**: Descarga tus conversaciones en JSON o Markdown

## ğŸš€ InstalaciÃ³n RÃ¡pida

### Prerrequisitos

- Python 3.8 o superior
- [Ollama](https://ollama.ai) instalado y ejecutÃ¡ndose
- 8GB RAM mÃ­nimo (16GB recomendado)

### Pasos de InstalaciÃ³n

1. **Clona el repositorio**
```bash
git clone https://github.com/tu-usuario/APU.git
cd APU
```



El instalador:
- âœ… VerificarÃ¡ tu entorno
- âœ… InstalarÃ¡ las dependencias
- âœ… ConfigurarÃ¡ las variables de entorno
- âœ… DescargarÃ¡ el modelo Llama 3.2:3b

1. **Configura Tavily (opcional)**

Para bÃºsquedas web, obtÃ©n una API key gratuita en [tavily.com](https://tavily.com) y agrÃ©gala al archivo `.env`:
```
TAVILY_API_KEY=tu_api_key_aqui
```

## ğŸ® Uso

1. **Inicia la aplicaciÃ³n**
```bash
python app.py
```

2. **Abre tu navegador**

Navega a [http://localhost:8501](http://localhost:8501)

3. **Carga tus documentos**

- Usa la barra lateral para cargar PDFs
- Los documentos se procesarÃ¡n automÃ¡ticamente
- VerÃ¡s las estadÃ­sticas actualizarse

4. **Â¡Comienza a preguntar!**

Ejemplos de preguntas:
- "Â¿QuÃ© dice el documento sobre machine learning?"
- "Resume la metodologÃ­a del paper"
- "Â¿CuÃ¡les son las conclusiones principales?"
- "Busca en internet informaciÃ³n actualizada sobre [tema]"

## ğŸ“ Estructura del Proyecto

```
APU/
â”œâ”€â”€ app.py              # AplicaciÃ³n principal Streamlit
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py     # Configuraciones centrales
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ document_processor.py  # Procesamiento de PDFs
â”‚   â”œâ”€â”€ embeddings.py         # GestiÃ³n de embeddings
â”‚   â””â”€â”€ vector_store.py       # AlmacÃ©n vectorial FAISS
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ main_agent.py   # Agente orquestador
â”‚   â””â”€â”€ memory.py       # GestiÃ³n de memoria
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ rag_tool.py     # Herramienta RAG
â”‚   â””â”€â”€ web_search_tool.py  # BÃºsqueda web
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ components.py   # Componentes de interfaz
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ logger.py       # Sistema de logging
â”‚   â””â”€â”€ helpers.py      # Funciones auxiliares
â””â”€â”€ data/
    â”œâ”€â”€ documents/      # PDFs originales
    â”œâ”€â”€ processed/      # Documentos procesados
    â””â”€â”€ faiss_index/    # Ãndice vectorial
```

## ğŸ› ï¸ ConfiguraciÃ³n Avanzada

### Variables de Entorno

Edita el archivo `.env` para personalizar:

```env
# Modelo de IA
OLLAMA_MODEL=llama3.2:3b
OLLAMA_HOST=http://localhost:11434

# Embeddings
EMBEDDINGS_MODEL=all-MiniLM-L6-v2

# Procesamiento
CHUNK_SIZE=500
CHUNK_OVERLAP=50

# UI
STREAMLIT_THEME=dark
MAX_CHAT_HISTORY=50
```

### Modelos Alternativos

Puedes usar otros modelos de Ollama:
```bash
# Modelos mÃ¡s grandes (mejor calidad)
ollama pull llama3.2:7b
ollama pull mistral:7b

# Modelos mÃ¡s pequeÃ±os (mÃ¡s rÃ¡pidos)
ollama pull phi3:mini
```

## ğŸ”§ SoluciÃ³n de Problemas

### Ollama no se conecta
```bash
# Verifica que Ollama estÃ© ejecutÃ¡ndose
ollama list

# Reinicia Ollama
ollama serve
```

### Error de memoria
- Reduce `CHUNK_SIZE` en `.env`
- Usa un modelo mÃ¡s pequeÃ±o
- Procesa menos documentos a la vez

### Documentos no se procesan
- Verifica que sean PDFs vÃ¡lidos
- Revisa los logs en `apu.log`
- AsegÃºrate de que tengan texto extraÃ­ble


## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver `LICENSE` para mÃ¡s detalles.

## ğŸ™ TecnologÃ­as principales

- [Langchain](https://langchain.com) por el framework de agentes
- [Streamlit](https://streamlit.io) por la interfaz web
- [Ollama](https://ollama.ai) por los modelos locales
- [FAISS](https://github.com/facebookresearch/faiss) por la bÃºsqueda vectorial

## ğŸ“ Contacto

Â¿Preguntas? Â¿Problemas? 
- ğŸ“§ Email: tu-email@ejemplo.com
- ğŸ› Issues: [GitHub Issues](https://github.com/tu-usuario/APU/issues)

---

